{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PATH='./data/nsmc/'\n",
    "\n",
    "train = pd.read_csv(DEFAULT_PATH+'ratings_train.txt', header=0, delimiter='\\t' ,quoting=3 )\n",
    "test = pd.read_csv(DEFAULT_PATH+'ratings_test.txt', header=0, delimiter='\\t' ,quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(review, okt, remove_stopwords = False, stop_words = []):\n",
    "    # 함수의 인자는 다음과 같다.\n",
    "    # review : 전처리할 텍스트\n",
    "    # okt : okt 객체를 반복적으로 생성하지 않고 미리 생성후 인자로 받는다.\n",
    "    # remove_stopword : 불용어를 제거할지 선택 기본값은 False\n",
    "    # stop_word : 불용어 사전은 사용자가 직접 입력해야함 기본값은 비어있는 리스트\n",
    "    \n",
    "    # 1. 한글 및 공백을 제외한 문자 모두 제거.\n",
    "    review_text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", review)\n",
    "    \n",
    "    # 2. okt 객체를 활용해서 형태소 단위로 나눈다.\n",
    "    word_review = okt.morphs(review_text, stem=True)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        \n",
    "        # 불용어 제거(선택적)\n",
    "        word_review = [token for token in word_review if not token in stop_words]\n",
    "        \n",
    "   \n",
    "    return word_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [ '은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', '주', '등', '한']\n",
    "okt = Okt()\n",
    "clean_review = []\n",
    "clean_review_test = []\n",
    "\n",
    "for review in train['document']:\n",
    "    # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
    "    if type(review) == str:\n",
    "        clean_review.append(preprocessing(review, okt, remove_stopwords = True, stop_words=stop_words))\n",
    "    else:\n",
    "        clean_review.append([])\n",
    "\n",
    "for review in test['document']:\n",
    "    # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
    "    if type(review) == str:\n",
    "        clean_review_test.append(preprocessing(review, okt, remove_stopwords = True, stop_words=stop_words))\n",
    "    else:\n",
    "        clean_review_test.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_review))\n",
    "print(len(clean_review_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 단어 개수:  43756\n",
      "Shape of input data tensor: (150000, 10)\n",
      "Shape of label tensor: (150000,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_review)\n",
    "text_sequences = tokenizer.texts_to_sequences(clean_review)\n",
    "\n",
    "word_vocab = tokenizer.word_index # 딕셔너리 형태\n",
    "print(\"전체 단어 개수: \", len(word_vocab)) # 전체 단어 개수 확인\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 10 # 문장 최대 길이\n",
    "\n",
    "inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post') # 문장의 길이가 50 단어가 넘어가면 자르고, 모자르면 0으로 채워 넣는다.\n",
    "labels = np.array(train['label']) # 각 리뷰의 감정을 넘파이 배열로 만든다.\n",
    "\n",
    "print('Shape of input data tensor:', inputs.shape) # 리뷰 데이터의 형태 확인\n",
    "print('Shape of label tensor:', labels.shape) # 감정 데이터 형태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 단어 개수:  26778\n",
      "Shape of input data tensor: (50000, 10)\n",
      "Shape of label tensor: (50000,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer_test = Tokenizer()\n",
    "tokenizer_test.fit_on_texts(clean_review_test)\n",
    "text_sequences_test = tokenizer_test.texts_to_sequences(clean_review_test)\n",
    "\n",
    "word_vocab_test = tokenizer_test.word_index # 딕셔너리 형태\n",
    "print(\"전체 단어 개수: \", len(word_vocab_test)) # 전체 단어 개수 확인\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 10 # 문장 최대 길이\n",
    "\n",
    "inputs_test = pad_sequences(text_sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post') # 문장의 길이가 50 단어가 넘어가면 자르고, 모자르면 0으로 채워 넣는다.\n",
    "labels_test = np.array(test['label']) # 각 리뷰의 감정을 넘파이 배열로 만든다.\n",
    "\n",
    "print('Shape of input data tensor:', inputs_test.shape) # 리뷰 데이터의 형태 확인\n",
    "print('Shape of label tensor:', labels_test.shape) # 감정 데이터 형태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 및 파일 이름 지정\n",
    "FILE_DIR_PATH = './data/'\n",
    "INPUT_TRAIN_DATA_FILE_NAME = 'nsmc_train_input.npy'\n",
    "LABEL_TRAIN_DATA_FILE_NAME = 'nsmc_train_label.npy'\n",
    "DATA_CONFIGS_FILE_NAME = 'data_configs.json'\n",
    "INPUT_TEST_DATA_FILE_NAME = 'nsmc_test_input.npy'\n",
    "LABEL_TEST_DATA_FILE_NAME = 'nsmc_test_label.npy'\n",
    "\n",
    "\n",
    "data_configs = word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab) # vocab size 추가\n",
    "\n",
    "\n",
    "# 저장하는 디렉토리가 존재하지 않으면 생성\n",
    "if not os.path.exists(FILE_DIR_PATH):\n",
    "    os.makedirs(FILE_DIR_PATH)\n",
    "\n",
    "# 전처리 된 데이터를 넘파이 형태로 저장\n",
    "np.save(open(FILE_DIR_PATH + INPUT_TRAIN_DATA_FILE_NAME, 'wb'), inputs)\n",
    "np.save(open(FILE_DIR_PATH + LABEL_TRAIN_DATA_FILE_NAME, 'wb'), labels)\n",
    "\n",
    "np.save(open(FILE_DIR_PATH + INPUT_TEST_DATA_FILE_NAME, 'wb'), inputs_test)\n",
    "np.save(open(FILE_DIR_PATH + LABEL_TEST_DATA_FILE_NAME, 'wb'), labels_test)\n",
    "\n",
    "# 데이터 사전을 json 형태로 저장\n",
    "json.dump(word_vocab, open(FILE_DIR_PATH + DATA_CONFIGS_FILE_NAME, 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
